{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Numpy Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    # assuming x.size() = [batch, numheads, sequence_length, dimension]\n",
    "    return (np.exp(x).transpose(-2, -1) / np.sum(np.exp(x), axis=-1)).transpose(-2, -1)\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1] # query vector dimension\n",
    "    scaled = np.matmul(q, k.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled + mask\n",
    "    attention = softmax(scaled)\n",
    "    out = np.matmul(attention, v)\n",
    "    return out, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `torch` - Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1] # query vector dimension\n",
    "    scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled + mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    out = torch.matmul(attention, v)\n",
    "    return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "sequence_length = 4 # My name is Goro\n",
    "input_dim = 512\n",
    "d_model = 512\n",
    "\n",
    "x = torch.randn(batch_size, sequence_length, input_dim)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query vector: what I am looking for?\n",
    "### Key vector: what I can offer\n",
    "### Value vector: what I actually offer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the query, key, value vectors\n",
    "qkv_layer = nn.Linear(input_dim, 3*d_model)\n",
    "qkv = qkv_layer(x)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 192])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_heads = 8\n",
    "head_dim = d_model // num_heads # Purpose of //: Floor division: returns the largest integer <= result. (7//2 = 3)\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3*head_dim) \n",
    "qkv.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv.permute(0, 2, 1, 3) # [batch_size, num_heads, sequence_length, 3*head_dim]\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide each tensor into 3 chunks on the last dimension\n",
    "q, k, v = qkv.chunk(3, dim=-1)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention for Multiple Heads\n",
    " \n",
    "- Scaled by sqrt(dk) is to **stablize the training** by lowering the **variance** of the dot product.\n",
    "- **Mask**: used in **decoder** only. This is to prevent prediction of word `i` by looking at future predictions `i+1` ~ ... `n`.\n",
    "- **Value** matrix: the **multiplier**.\n",
    "\n",
    "\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mtext>self attention</mtext>\n",
    "  <mo>=</mo>\n",
    "  <mi>s</mi>\n",
    "  <mi>o</mi>\n",
    "  <mi>f</mi>\n",
    "  <mi>t</mi>\n",
    "  <mi>m</mi>\n",
    "  <mi>a</mi>\n",
    "  <mi>x</mi>\n",
    "  <mrow data-mjx-texclass=\"ORD\">\n",
    "    <mo minsize=\"2.047em\" maxsize=\"2.047em\">(</mo>\n",
    "  </mrow>\n",
    "  <mfrac>\n",
    "    <mrow>\n",
    "      <mi>Q</mi>\n",
    "      <mo>.</mo>\n",
    "      <msup>\n",
    "        <mi>K</mi>\n",
    "        <mi>T</mi>\n",
    "      </msup>\n",
    "    </mrow>\n",
    "    <msqrt>\n",
    "      <msub>\n",
    "        <mi>d</mi>\n",
    "        <mi>k</mi>\n",
    "      </msub>\n",
    "    </msqrt>\n",
    "  </mfrac>\n",
    "  <mo>+</mo>\n",
    "  <mi>M</mi>\n",
    "  <mrow data-mjx-texclass=\"ORD\">\n",
    "    <mo minsize=\"2.047em\" maxsize=\"2.047em\">)</mo>\n",
    "  </mrow>\n",
    "</math>\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mtext>new V</mtext>\n",
    "  <mo>=</mo>\n",
    "  <mtext>self attention</mtext>\n",
    "  <mo>.</mo>\n",
    "  <mi>V</mi>\n",
    "</math>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = q.size()[-1]\n",
    "scaled = torch.matmul(q, k.transpose(-2,-1))\n",
    "mask = torch.full(scaled.size(), float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "# For decoder\n",
    "attention = F.softmax(scaled + mask, dim=-1)\n",
    "values = torch.matmul(attention, v)\n",
    "values.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size(): torch.Size([30, 10, 1024])\n",
      "    q size: torch.Size([30, 8, 10, 64])\n",
      "    k size: torch.Size([30, 8, 10, 64])\n",
      "    v size: torch.Size([30, 8, 10, 64]), \n",
      "values.size(): torch.Size([30, 8, 10, 64]), attention.size:torch.Size([30, 8, 10, 10]) \n",
      "out.size() torch.Size([30, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(input_dim, 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        print(f\"x.size(): {x.size()}\")\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v, = qkv.chunk(3, dim=-1)\n",
    "        print(f\"    q size: {q.size()}\\n    k size: {k.size()}\\n    v size: {v.size()}, \")\n",
    "        values, attention = scaled_dot_product_attention(q, k, v, mask)\n",
    "        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
    "        # Concatenate values\n",
    "        values = values.reshape(batch_size, sequence_length, num_heads * self.head_dim)\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"out.size() {out.size()}\")\n",
    "        return out\n",
    "\n",
    "# Input\n",
    "input_dim = 1024\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "batch_size = 30\n",
    "sequence_length = 10\n",
    "x = torch.randn(batch_size, sequence_length, input_dim)\n",
    "\n",
    "model = MultiHeadAttention(input_dim, d_model, num_heads)\n",
    "out = model.forward(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 10\n",
    "d_model = 6 # dimension of embedding. Typically 512\n",
    "\n",
    "even_i = torch.arange(0, d_model, 2).float()\n",
    "even_denominator = torch.pow(10000, even_i/d_model)\n",
    "\n",
    "odd_i = torch.arange(1, d_model, 2).float()\n",
    "odd_denominator = torch.pow(10000, odd_i/d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denominator = even_denominator\n",
    "position = torch.arange(max_sequence_length, dtype=torch.float).reshape(max_sequence_length, 1)\n",
    "even_PE = torch.sin(position/even_denominator)\n",
    "odd_PE = torch.sin(position/odd_denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 8.4147e-01,  2.1378e-01,  4.6399e-02,  9.9998e-03,  2.1544e-03,\n",
       "          4.6416e-04],\n",
       "        [ 9.0930e-01,  4.1768e-01,  9.2698e-02,  1.9999e-02,  4.3089e-03,\n",
       "          9.2832e-04],\n",
       "        [ 1.4112e-01,  6.0226e-01,  1.3880e-01,  2.9995e-02,  6.4633e-03,\n",
       "          1.3925e-03],\n",
       "        [-7.5680e-01,  7.5900e-01,  1.8460e-01,  3.9989e-02,  8.6176e-03,\n",
       "          1.8566e-03],\n",
       "        [-9.5892e-01,  8.8064e-01,  2.3000e-01,  4.9979e-02,  1.0772e-02,\n",
       "          2.3208e-03],\n",
       "        [-2.7942e-01,  9.6157e-01,  2.7491e-01,  5.9964e-02,  1.2926e-02,\n",
       "          2.7850e-03],\n",
       "        [ 6.5699e-01,  9.9804e-01,  3.1922e-01,  6.9943e-02,  1.5080e-02,\n",
       "          3.2491e-03],\n",
       "        [ 9.8936e-01,  9.8836e-01,  3.6285e-01,  7.9915e-02,  1.7235e-02,\n",
       "          3.7133e-03],\n",
       "        [ 4.1212e-01,  9.3298e-01,  4.0570e-01,  8.9879e-02,  1.9389e-02,\n",
       "          4.1774e-03]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        even_denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
    "        even_PE = torch.sin(position/even_denominator)\n",
    "        odd_i = torch.arange(1, d_model, 2).float()\n",
    "        odd_denominator = torch.pow(10000, odd_i/d_model)        \n",
    "        odd_PE = torch.cos(position/odd_denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        return PE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.9769,  0.0464,  0.9999,  0.0022,  1.0000],\n",
       "        [ 0.9093,  0.9086,  0.0927,  0.9998,  0.0043,  1.0000],\n",
       "        [ 0.1411,  0.7983,  0.1388,  0.9996,  0.0065,  1.0000],\n",
       "        [-0.7568,  0.6511,  0.1846,  0.9992,  0.0086,  1.0000],\n",
       "        [-0.9589,  0.4738,  0.2300,  0.9988,  0.0108,  1.0000],\n",
       "        [-0.2794,  0.2746,  0.2749,  0.9982,  0.0129,  1.0000],\n",
       "        [ 0.6570,  0.0627,  0.3192,  0.9976,  0.0151,  1.0000],\n",
       "        [ 0.9894, -0.1522,  0.3629,  0.9968,  0.0172,  1.0000],\n",
       "        [ 0.4121, -0.3599,  0.4057,  0.9960,  0.0194,  1.0000]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionalEncoding(d_model=6, max_sequence_length=10)\n",
    "pe.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.Tensor([  # batch size: 1\n",
    "    [[0.2, 0.1, 0.3],    # 2 words\n",
    "     [0.5, 0.1, 0.1]]    # each word - 3 dimensinoal embedding\n",
    "])\n",
    "\n",
    "B, S, E = inputs.size() # Batch size, Sequence length, Embedding dimension\n",
    "inputs = inputs.reshape(S, B, E)\n",
    "inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_shape =inputs.size()[-2:] # batch x embedding dimension\n",
    "gamma = nn.Parameter(torch.ones(parameter_shape))\n",
    "beta = nn.Parameter(torch.zeros(parameter_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -2]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = [-(i+1) for i in range(len(parameter_shape))]\n",
    "dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean.size() torch.Size([2, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0817]],\n",
       "\n",
       "        [[0.1886]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = inputs.mean(dim=dims, keepdim=True)\n",
    "print(f\"mean.size() {mean.size()}\")\n",
    "var = ((inputs-mean)**2).mean(dim=dims, keepdim=True)\n",
    "epsilon = 1e-5\n",
    "std = (var + epsilon).sqrt()\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (inputs-mean) / std\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
       "\n",
       "        [[ 1.4140, -0.7070, -0.7070]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = gamma * y + beta\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization():\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        self.parameters_shape = parameters_shape\n",
    "        print(f\"self.parameters_shape: {self.parameters_shape}\")\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(self.parameters_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(self.parameters_shape))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i+1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        print(f\"mean size: {mean.size()} one mean for each word\")\n",
    "        var = ((inputs-mean)**2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.parameters_shape: torch.Size([3, 8])\n",
      "mean size: torch.Size([5, 1, 1]) one mean for each word\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 8])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "sequence_length = 5 # 5 words\n",
    "embedding_dim = 8\n",
    "inputs = torch.randn(batch_size, sequence_length, embedding_dim)\n",
    "inputs = inputs.permute(1, 0, 2) # S, B, E\n",
    "# print(f\"inputs \\n ({inputs.size()}) = \\n {inputs}\")\n",
    "\n",
    "layer_norm = LayerNormalization(inputs.size()[-2:])\n",
    "out = layer_norm.forward(inputs)\n",
    "out.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "The goal of **encoder** is to transform the input sequence (collection of words) into embeddings (vectors) that better **encapsulate** the **context of the words**.\n",
    "\n",
    "Better representations of the **meaning** of the words.\n",
    "\n",
    "Be used in the decoder to assist in **translation**\n",
    "\n",
    "### How? -> Encoder layer\n",
    "\n",
    "### Residual/skip Connections (to Add & Norm): Avoid the vanishing gradient problem\n",
    "- For **very deep networks**, the backpropagation of values will eventually be **very small gradients**. (ReLU, ... -> near 0 activation)\n",
    "- The network **stop learning** if gradients become super small.\n",
    "- To prevent the vanishing gradients problem: Use **skip connection**\n",
    "\n",
    "### Why Layer Normalization? Want to preform more stable training\n",
    "- Naturally, the values after positional encoding would have scattered means and large standard deviations.\n",
    "- LayerNorm ensure values are centered around 0 with std ~ 1. \n",
    "    - Backprop: gets more even steps during the learning process\n",
    "- Training becomes more stable, easier, and faster\n",
    "\n",
    "Output after the **encoder block**: Better contextual awareness.\n",
    "\n",
    "After vectors are passed through the network encompassing attention\n",
    "1. Preserve signals via skip connections.\n",
    "2. Much more stable values via layer normalizations.\n",
    "\n",
    "Overall, each vector better represents the word compared to the original input vector.\n",
    "\n",
    "Original transformer paper uses a stack of **N=6** encoder blocks cascaded one after the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
